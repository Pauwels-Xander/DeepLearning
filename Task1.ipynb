{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpZynWPmOPUysY3M0ktBpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pauwels-Xander/DeepLearning/blob/main/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrejcXvyBvXn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Exercise 1 link: https://pdf.ac/14Itnp\n",
        "\n",
        "def generate_sin(x, epsilon):\n",
        "    func = np.sin(x) + np.random.normal(loc=0.0, scale=epsilon, size=len(x))\n",
        "    return func\n",
        "\n",
        "n_samples = 20\n",
        "epsilon = 0.1\n",
        "\n",
        "# TODO: create noisy function\n",
        "x = np.linspace(0, np.pi/4, n_samples)\n",
        "y = generate_sin(x, epsilon)\n",
        "\n",
        "# TODO: create true function\n",
        "x_pure = x\n",
        "y_pure = np.sin(x)\n",
        "\n",
        "# TODO: plot data, line\n",
        "plt.scatter(x,y, label=\"Sampled data\")\n",
        "plt.plot(x_pure, y_pure, label=\"true\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -------------- Q2.2 -------------- #\n",
        "def compute_sos(y_true, y_pred):\n",
        "    sos = np.sum((y_true - y_pred)**2)\n",
        "    return sos\n",
        "\n",
        "def lin_reg(xs, ys):\n",
        "    n = len(xs)\n",
        "\n",
        "    x_mean = 1/n * np.sum(xs)\n",
        "    y_mean = 1/n * np.sum(ys)\n",
        "\n",
        "    b = np.sum((xs - x_mean) * (ys - y_mean)) / np.sum((xs - x_mean) ** 2)\n",
        "    a = y_mean - b*x_mean\n",
        "\n",
        "    y_pred = a + b*xs\n",
        "    res = compute_sos(ys, y_pred)\n",
        "\n",
        "    return a, b, res\n",
        "\n",
        "# TODO: generate line of best fit\n",
        "a, b, res = lin_reg(x,y)\n",
        "y_best = b*x+a\n",
        "\n",
        "\n",
        "# TODO: plot line of best fit, data, etc\n",
        "plt.scatter(x, y, label=\"Noisy Data\", color=\"red\")\n",
        "plt.plot(x_pure, y_pure, label=\"True Function (sin x)\", color=\"blue\", linestyle=\"dashed\")\n",
        "plt.plot(x, y_best, label=\"Line of Best Fit\", color=\"green\")\n",
        "\n",
        "# make it pretty\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Coefficients:\", (a, b))\n",
        "print(\"Residuals:   \", res)\n",
        "\n",
        "\n",
        "# -------------- Q2.3 -------------- #\n",
        "# TODO: use np polyfit here\n",
        "b_np, a_np = np.polyfit(x, y, 1)\n",
        "y_best_np = a_np + b_np * x\n",
        "res_np = compute_sos(y, y_best_np)\n",
        "\n",
        "# TODO: plot everything you need to\n",
        "plt.scatter(x, y, label=\"Noisy Data\", color=\"red\")\n",
        "plt.plot(x_pure, y_pure, label=\"True Function (sin x)\", color=\"blue\", linestyle=\"dashed\")\n",
        "plt.plot(x, y_best, label=\"Manual Linear Regression\", color=\"green\")\n",
        "plt.plot(x, y_best_np, label=\"NumPy Polyfit Regression\", color=\"purple\", linestyle=\"dotted\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# print results\n",
        "print(\"Coefficients:     \", (a, b))\n",
        "print(\"Residuals:        \", res)\n",
        "print(\"Coefficients (np):\", (a_np, b_np))\n",
        "print(\"Residuals (np):   \", res_np)\n",
        "# polyfit provides a fast and convenient way to compute linear regression\n",
        "# without manually implementing the formula, as they output the same.\n",
        "\n",
        "# -------------- Q3 -------------- #\n",
        "def compute_mse(y_true, y_pred, a, b):\n",
        "    mse = 1/len(y_true) * np.sum((y_true-y_pred)**2)\n",
        "    return mse\n",
        "\n",
        "x_test = np.linspace(0, np.pi/4, n_samples)\n",
        "y_test = np.sin(x_test) + np.random.normal(loc=0.0, scale=epsilon, size=len(x))\n",
        "\n",
        "plt.scatter(x, y, label=\"Train set)\")\n",
        "plt.scatter(x_test, y_test, label=\"Test set\")\n",
        "\n",
        "# TODO: plot data, lines of best fit, true function, make it pretty\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x, y, label=\"Train set\", color='blue', alpha=0.6)\n",
        "plt.scatter(x_test, y_test, label=\"Test set\", color='red', alpha=0.6)\n",
        "plt.plot(x, a * x + b, label=f'Best Fit Line: y = {a:.2f}x + {b:.2f}', color='black')\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Linear Regression Fit with Training and Test Sets\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "test_loss = compute_mse(x_test, y_test, a, b)\n",
        "train_loss = compute_mse(x, y, a, b)\n",
        "\n",
        "print(\"Coefficients: \", (a, b))\n",
        "print(\"Train loss:   \", train_loss)\n",
        "print(\"Test loss:    \", test_loss)\n",
        "\n",
        "# The test loss is lower. The training loss is typically lower than the test loss because\n",
        "# the model is optimized to fit the training data. So this contradicts.\n",
        "# This may be because the test set is too small or not representative.\n",
        "# 0.008 (train) vs 0.001 (test)\n",
        "\n",
        "# -------------- Q4 -------------- #\n",
        "# Observation 1: Increase in noise increases train loss significantly\n",
        "# Observation 2: Increase in sample size decreases test loss\n",
        "\n",
        "# -------------- Q5.1 -------------- #\n",
        "# TODO: generate new data and plot\n",
        "n_samples = 20\n",
        "x = np.linspace(0, 2* np.pi, n_samples)\n",
        "epsilon = 0.2\n",
        "x_train = np.linspace(0, np.pi / 4, n_samples)  # Training set range\n",
        "x_test = np.linspace(np.pi / 4, np.pi / 2, n_samples)  # Test set range\n",
        "\n",
        "y_train = generate_sin(x_train, epsilon)\n",
        "y_test = generate_sin(x_test, epsilon)\n",
        "\n",
        "x_pure = np.linspace(0, np.pi / 2, 100)  # Smooth curve for visualization\n",
        "y_pure = np.sin(x_pure)\n",
        "\n",
        "plt.scatter(x_train, y_train, label=\"Train Set\", color=\"blue\", alpha=0.6)\n",
        "plt.scatter(x_test, y_test, label=\"Test Set\", color=\"red\", alpha=0.6)\n",
        "plt.plot(x_pure, y_pure, label=\"True f(x): sin(x)\", color=\"black\", linestyle=\"dashed\")\n",
        "\n",
        "# Make plot visually appealing\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Training and Test Sets with True Function\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# -------------- Q5.2 -------------- #\n",
        "degrees = range(1, 20)\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "coefficients = []\n",
        "\n",
        "for d in degrees:\n",
        "    # Fit polynomial model to training data\n",
        "    poly_coeffs = np.polyfit(x_train, y_train, d)\n",
        "\n",
        "    # Compute predictions on training and test sets\n",
        "    y_pred_train = np.polyval(poly_coeffs, x_train)\n",
        "    y_pred_test = np.polyval(poly_coeffs, x_test)\n",
        "\n",
        "    # Compute train and test losses using Mean Squared Error (MSE)\n",
        "    train_loss = compute_mse(y_train, y_pred_train, 0, 0)\n",
        "    test_loss = compute_mse(y_test, y_pred_test, 0, 0)\n",
        "\n",
        "    # Store results\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    coefficients.append(poly_coeffs)\n",
        "\n",
        "df_poly_results = pd.DataFrame({\n",
        "    \"Degree\": degrees,\n",
        "    \"Train Loss\": train_losses,\n",
        "    \"Test Loss\": test_losses,\n",
        "    \"Coefficients\": coefficients\n",
        "})\n",
        "print(df_poly_results)\n",
        "\n",
        "# For low-degree polynomials, the test loss is moderate, and train loss is relatively low.\n",
        "# For higher-degree polynomials the test loss starts increasing significantly,\n",
        "# which suggests overfitting—the model fits the training data too well but generalizes poorly.\n",
        "\n",
        "# -------------- Q5.3 -------------- #\n",
        "train_losses_actual = train_losses\n",
        "test_losses_actual = test_losses\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Overfitting plot: Training loss decreases rapidly, validation loss worsens\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(degrees, train_losses_actual, label='Training Loss', color=\"blue\", marker=\"o\")\n",
        "plt.plot(degrees, test_losses_actual, label='Validation Loss', color=\"red\", marker=\"s\")\n",
        "plt.title(\"Overfitting: Large gap between training and validation loss\")\n",
        "plt.xlabel(\"Polynomial Degree\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Underfitting plot: Both training and validation loss remain high\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(degrees, train_losses_actual, label='Training Loss', color=\"blue\", marker=\"o\")\n",
        "plt.plot(degrees, test_losses_actual, label='Validation Loss', color=\"red\", marker=\"s\")\n",
        "plt.title(\"Underfitting: High loss in both training and validation\")\n",
        "plt.xlabel(\"Polynomial Degree\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Plot MSE losses vs. polynomial degree\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(degrees, train_losses_actual, label=\"Training Loss\", marker='o', linestyle='-', color='blue')\n",
        "plt.plot(degrees, test_losses_actual, label=\"Test Loss\", marker='s', linestyle='-', color='red')\n",
        "\n",
        "plt.xlabel(\"Polynomial Degree (Model Flexibility)\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title(\"MSE vs. Model Flexibility\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# -------------- Q6 -------------- #\n",
        "# a) Where do the polynomial models start to overfit? How can you tell?\n",
        "# It is hard to see on the plots, as the problem grows exponentially. So impossible to tell from the plots.\n",
        "# Manually looking, probably around d=5 as the MSE grows too big.\n",
        "\n",
        "# b) What model fits the best to the training data? What model fits the best to the underlying function (visually, according to your intuition about the plots)? Are they the same?\n",
        "# The high degree one does. But a moderate degree does better capture the underlying function because it doesnt overextense\n",
        "# to account for noise.\n",
        "\n",
        "# c) Try increasing the number of training points to 200. At what point does overfitting start now?\n",
        "# around d=10\n",
        "\n",
        "# d) In general, does increasing the number of training points always allow for more flexible models of the real world? Why/why not?\n",
        "# Increasing the number of training points can help models generalize better and delay overfitting, but it doesn’t always allow for\n",
        "# more flexibility. If the true function is simple or the data is noisy, adding more samples won’t improve model performance and\n",
        "# may lead to diminishing returns.\n"
      ]
    }
  ]
}